{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595310a0-37b3-4968-8fad-547b468c6e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "import stanza\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1e769-3f25-424b-8ac6-7ec53403979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Simaligner\n",
    "from simalign import SentenceAligner\n",
    "\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b77300-dd29-4841-8fea-80fb8c0b1877",
   "metadata": {},
   "source": [
    "# Translog to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f61730-3f37-4b98-b518-b402073d3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn = \"/data/critt/tprdb/TPRDB/AR22/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/SG12/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Translog-II/P01_T1.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/RUC17/Translog-II/P01_T1.xml\"\n",
    "#fn = \"/data/critt/tprdb/TPRDB/STC17bolt/Translog-II/P01_T1.xml\"\n",
    "#fn = \"/data/critt/tprdb/TPRDB/ENJA15/Translog-II/P01_T1.xml\"\n",
    "\n",
    "wks_root = Translog2WKS(fn)\n",
    "\n",
    "# pretty-print WorkSpace_root\n",
    "ET.indent(wks_root, space='  ')  # 2 spaces\n",
    "print(ET.tostring(wks_root, encoding='unicode'))\n",
    "\n",
    "## to do\n",
    "# Realign and substitute in WKS\n",
    "# Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e65671-ff63-4dc6-98ef-cec20d14d85a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Translog2WKS(fn) :\n",
    "    \n",
    "    ### Root of the WorkSpace XML output file\n",
    "    WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "    \n",
    "    ### Read Translog-XML file\n",
    "    translog = ET.parse(fn)\n",
    "    # Root of the Translog XML input file\n",
    "    translog_root = translog.getroot()\n",
    "    \n",
    "    # get Source and Target Languages\n",
    "    e = translog_root.find('.//Languages')\n",
    "    SL = e.get('source') \n",
    "    TL = e.get('target')\n",
    "    \n",
    "    ##################################################################\n",
    "    ### ST segmentation and tokenization\n",
    "    # <SourceToken language=\"en\" >\n",
    "    #    <Token cur=\"0\" tokId=\"1\" pos=\"NNP\" sntId=\"1\" tok=\"Killer\" />\n",
    "      \n",
    "    # Source text and target Text \n",
    "    SText = getSourceText(translog_root)\n",
    "    \n",
    "    # segment and tokenize the source text\n",
    "    (STsnt, SToken, SToken_root) = Tokenize(SText, SL, 'SourceToken')\n",
    "    \n",
    "    # append ST tokenization to WorkSpace root \n",
    "    WorkSpace_root.append(SToken_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### TT final target text (translation) segmentation and tokenization\n",
    "    #  <FinalToken language=\"ar\" >\n",
    "    #    <Token  cur=\"0\" tokId=\"1\" sntId=\"1\" tok=\"ﺎﻠﻤﻣﺮﺿ\" />  \n",
    "    \n",
    "    # get final text from Translog file \n",
    "    FText = getFinalText(translog_root)\n",
    "    \n",
    "    # segment and tokenize the target text\n",
    "    (FTsnt, FToken, FToken_root) = Tokenize(FText, TL, 'FinalToken')\n",
    "    \n",
    "    # append FT tokenization to WorkSpace root \n",
    "    WorkSpace_root.append(FToken_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment-alignment\n",
    "    #  <SntAlignment>\n",
    "    #     <Snt src=\"1\" tgt=\"1\" />\n",
    "    \n",
    "    # very preliminary Sentence Alignment\n",
    "    SntAlignList = sntAlignment(STsnt, FTsnt)\n",
    "    \n",
    "    # convert SntAlign Dictionary to xml\n",
    "    SntAln_root = list_of_dicts_to_xml(SntAlignList, root_tag='SntAlignment', item_tag='Snt')\n",
    "    \n",
    "    # append SntAlign Dictionary to WorkSpace root \n",
    "    WorkSpace_root.append(SntAln_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### Token-alignment\n",
    "    #  <TokAlign>\n",
    "    #     <Tok sid=\"1\" tid=\"1\" />\n",
    "\n",
    "    # create segment alignments\n",
    "    segAlign = snt2segAlign(STsnt, FTsnt, SntAlignList)\n",
    "    \n",
    "    # random token alignment: not very useful\n",
    "#    tokAlign = rndAlignment(segAlign)\n",
    "    \n",
    "    tokAlign = simAlignment(segAlign)\n",
    "    \n",
    "    # convert tokAlign Dictionary to xml\n",
    "    TokAln_root = list_of_dicts_to_xml(tokAlign, root_tag='TokAlignment', item_tag='Tok')\n",
    "    \n",
    "    # append SntAlign Dictionary to WorkSpace root \n",
    "    WorkSpace_root.append(TokAln_root)\n",
    "\n",
    "    ##################################################################\n",
    "    ### Keystroke-Token mapping\n",
    "    #  <Modifications>\n",
    "    #    <Mod time=\"72953\" type=\"Mins\" cur=\"0\" chr=\"ﺍ\" X=\"0\" Y=\"0\" sntId=\"1\" sid=\"2\" tid=\"1\"  />\n",
    "     \n",
    "    ##################################################################\n",
    "    ### Fixation-Token mapping\n",
    "    #  <Fixations>\n",
    "    #      <Fix time=\"30\" win=\"1\" cur=\"227\" dur=\"175\" X=\"502\" Y=\"228\" sntId=\"3\" sid=\"41\" tid=\"39\" />\n",
    "    \n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment open-closing\n",
    "    #  <Segments>\n",
    "    #    <Seg sntId=\"1\" open=\"72952\" close=\"89436\" />\n",
    "\n",
    "    return WorkSpace_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ec3d0-3af4-4df6-bbe4-9d4cad3d655a",
   "metadata": {},
   "source": [
    "## Linguistic processing\n",
    "- sentence segmentation (NLTK)\n",
    "- tokenization (NLTK)\n",
    "- lexical features (Stanza)\n",
    "- cursor offset of words in text\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b55e-049d-413e-b554-9507a7c03240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(text, lng, tag, form=1, stanzaFeats=1):\n",
    "    \"\"\"\n",
    "    Tokenize and annotate text with linguistic features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw input text to process\n",
    "    lng : str\n",
    "        Language code (e.g., 'en', 'es', 'de')\n",
    "    tag : str\n",
    "        XML root tag name (e.g., 'SourceText', 'TargetText')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (snt, toksFeats, token_root)\n",
    "        - snt: List of tokenized sentences with POS tags\n",
    "        - toksFeats: List of token dictionaries with all features\n",
    "        - token_root: XML ElementTree with all token data\n",
    "    \"\"\"\n",
    "    \n",
    "    # segment and tokenize source text \n",
    "    # snt: is list of tokenized ,Tagged sentences: \n",
    "    #    [[(token, pos), ...], [(token, pos), ... ], ...]  \n",
    "    snt = segmentText(text, lng=lng, form=form)\n",
    "\n",
    "    # create list of tokens with sntId, tokId, cursor offset\n",
    "    #    [{tok1features}, {tok2features}, ...]\n",
    "    toksList = tokenCurOffset(text, snt)\n",
    "\n",
    "    # get additional features from Stanza to list of tokens\n",
    "    # add features to list of STokens \n",
    "    if(stanzaFeats) :\n",
    "        tokens = stanzaFeatures(snt, lng, toksList)\n",
    "    else: tokens = toksList\n",
    "    \n",
    "    # convert token Dictionary to xml\n",
    "    token_root = list_of_dicts_to_xml(tokens, root_tag=tag, item_tag='Tok')\n",
    "    \n",
    "    # assign source language \n",
    "    token_root.set('language', str(lng))\n",
    "\n",
    "    return (snt, tokens, token_root)\n",
    "\n",
    "#########################################################################\n",
    "# get ST from the Translog file\n",
    "def getSourceText(root):\n",
    "    \n",
    "    # get text from UTF8 container in the xml file \n",
    "    ST = root.find('.//SourceTextUTF8')\n",
    "    if ST is not None:\n",
    "        return ST.text\n",
    "        \n",
    "    # in older versions there is no UTF8 version in the xml file \n",
    "    # else SourceTextChar must extist\n",
    "    text2 = ''\n",
    "    STchars = root.findall('.//SourceTextChar/CharPos')\n",
    "    for chars in STchars:\n",
    "        text2 += chars.get('Value')\n",
    "    return text2\n",
    "\n",
    "# get FT from the Translog file\n",
    "def getFinalText(root):\n",
    "\n",
    "    # FinalText in UTF8 should usually always be there\n",
    "    FT = root.find('.//FinalText')\n",
    "    if FT is not None:\n",
    "        return FT.text\n",
    "\n",
    "    # else FinalTextChar must extist\n",
    "    text2 = ''\n",
    "    FTchars = root.findall('.//FinalTextChar/CharPos')\n",
    "    for chars in FTchars:\n",
    "        text2 += chars.get('Value')\n",
    "    return text2\n",
    "\n",
    "#########################################################################\n",
    "# segment text FT from the Translog file\n",
    "def segmentText(text, lng='en', form = 1):\n",
    "\n",
    "    '''\n",
    "    form 0: Output Format: [['token', ...], ...] : Use Case: Just tokenized sentences\n",
    "    form 1: Output Format: [[('token', 'POS'), ...], ...]: Use Case: With POS tags (default)\n",
    "    form 2: Output Format: ['token token ...', ...]: Use Case: Sentences as strings\n",
    "    form 4: Uses Stanza for all languages: Use Case: High-quality annotations\n",
    "    '''\n",
    "    \n",
    "    # replace multiple \\n by one (no impact on NLTK segmentation)\n",
    "    text1 = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    if(form == 4) : return segmentStanza(text, lng)\n",
    "    if(lng == 'ja') : return segmentStanza(text, lng)\n",
    "    if(lng == 'zh') : return segmentChineseJieba(text, form)\n",
    "        \n",
    "    # Segment text into list of sentences\n",
    "    snt0 = sent_tokenize(text1)\n",
    "\n",
    "    # segment text at newline into segments (not covered by NLTK)\n",
    "    snt1 = []\n",
    "    for i in range(len(snt0)) :\n",
    "        s = snt0[i]\n",
    "        snt1.extend(s.split('\\n'))\n",
    "        \n",
    "    # Tokenize each sentences\n",
    "    snt1 = [word_tokenize(s) for s in snt1]\n",
    "    \n",
    "    # remove empty sentences (e.g. produced by \\n)\n",
    "    if([] in snt1): snt1.remove([])\n",
    "\n",
    "    # Part-of-speech tagging each sentences: works only properly for English\n",
    "    if(form == 1) : snt1 = [pos_tag(s) for s in snt1]\n",
    "\n",
    "    # collapse back into list of sentences\n",
    "    if(form == 2) : snt1 = [\" \".join(s) for s in snt1]\n",
    "        \n",
    "    return snt1\n",
    "\n",
    "\n",
    "def segmentStanza(text, lng) :\n",
    "#    pattern = r'[。！？]'\n",
    "\n",
    "    nlp = stanza.Pipeline(lang=lng, processors='tokenize,pos')\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # stanza document to list of list of dictionaries\n",
    "    stza_list = doc.to_dict()   \n",
    "    \n",
    "    L = []\n",
    "    for snt in stza_list:\n",
    "        T = []\n",
    "        for tok in snt :\n",
    "            T.append((tok['text'], tok['upos']))\n",
    "        if(len(T) > 0):  L.append(T)\n",
    "    return L\n",
    "\n",
    "def segmentChineseJieba(text, form) :\n",
    "    \n",
    "    words = pseg.cut(text)\n",
    "    return words2snt(list(words), form)\n",
    "\n",
    "def words2snt(words, form) :\n",
    "\n",
    "    pattern = r'[。！？.!?]'\n",
    "    \n",
    "    S = []\n",
    "    L = []\n",
    "    for tok, pos in words:\n",
    "        # skip word that are whitespaces\n",
    "        if(re.search(r'\\s',  tok)) : \n",
    "            # new sentence with \\n,but it's not a token\n",
    "            if(re.search(r'\\n',  tok)) : \n",
    "                if(len(L) > 0) : S.append(L)\n",
    "                L = []\n",
    "            continue\n",
    "            \n",
    "        # token and pos\n",
    "        if(form == 1) : L.append((tok, pos))\n",
    "        # only token if form != 1\n",
    "        else : L.append(tok)\n",
    "\n",
    "        # end of sentence\n",
    "        match = re.search(pattern, tok)\n",
    "        if(match) : \n",
    "            if(len(L) > 0) : S.append(L)\n",
    "            L = []\n",
    "\n",
    "    if(len(L) > 0) : S.append(L)\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "# additional features from Stanza\n",
    "def stanzaFeatures(snt, lng, token, processor='tokenize,mwt,pos,lemma', tokenize_no_ssplit=False, verbose=False):\n",
    "\n",
    "    # mwt not available for those languages\n",
    "    if (lng == 'ja' or lng == 'zh' or lng == 'ko' or lng == 'hi' or lng == 'nl'): processor = 'tokenize,pos,lemma'\n",
    "\n",
    "    # initialize stanza pipeline\n",
    "    nlp = stanza.Pipeline(lang=lng, processors=processor, tokenize_pretokenized=True, tokenize_no_ssplit=tokenize_no_ssplit, verbose=verbose)\n",
    "    \n",
    "    # keep only token from the list of sentences\n",
    "    # there can be empty tokens '' which are substituted by '.'\n",
    "    sntList = [[w if w != '' else '.' for w, p in s] for s in snt]\n",
    "\n",
    "    doc = nlp(sntList)\n",
    "\n",
    "    # stanza document to list of list of dictionaries\n",
    "    stza_list = doc.to_dict()\n",
    "    \n",
    "    # map list of NLTK tokens into dictinary for faster lookup \n",
    "    TD = {d['tokId']: d for d in token}\n",
    "\n",
    "    sntId = 0\n",
    "    tokId = 0\n",
    "    Token = []\n",
    "    off = 0\n",
    "\n",
    "    for snt in stza_list:\n",
    "        sntId +=1\n",
    "        for tok in snt :\n",
    "            tokId +=1\n",
    "            tok['sntId'] = sntId\n",
    "            tok['tokId'] = tokId\n",
    "            \n",
    "            # these features must be identical\n",
    "            if(tok['text'] != TD[tokId]['tok']) :\n",
    "                print(f\"stanzaFeatures Warning: snt:{sntId} tokId:{tokId} stanzaWord:>{tok['text']}< NLTKWord:>{TD[tokId]['tok']}<\")\n",
    "                      \n",
    "            # copy from tokList\n",
    "            tok['cur'] = TD[tokId]['cur']\n",
    "            tok['tok'] = TD[tokId]['tok']\n",
    "            \n",
    "            if('space' in TD[tokId]):  tok['space'] = TD[tokId]['space']\n",
    "            else:  tok['space'] = ''\n",
    "\n",
    "            # pos tag from NLTK\n",
    "            if('pos' in TD[tokId]): tok['pos'] = TD[tokId]['pos']\n",
    "\n",
    "            # delete Stanza features\n",
    "            if('text' in tok): tok.pop('text')\n",
    "            if('misc' in tok): tok.pop(\"misc\")\n",
    "            if('id' in tok): tok.pop(\"id\")\n",
    "            if('start_char' in tok): tok.pop(\"start_char\")\n",
    "            if('end_char' in tok): tok.pop(\"end_char\")\n",
    "            Token.append(tok)\n",
    "    return Token\n",
    "\n",
    "\n",
    "# Find cursor offset for tokens in text\n",
    "def tokenCurOffset(text, snt): \n",
    "    \n",
    "    L = [] # list of dictionaries the contain Token information\n",
    "    end = 0 # position of end of previous word in text\n",
    "    tokId = 0  # word ID\n",
    "    sntId = 0  # sentence ID\n",
    "\n",
    "    for s in snt:\n",
    "        sntId += 1\n",
    "        #for tok, pos in s:\n",
    "        for i in range(len(s)):\n",
    "            tok, pos = s[i]\n",
    "            start = text[end:].find(tok)\n",
    "            space = text[end:end+start]\n",
    "            cur = end+start \n",
    "            tokId += 1\n",
    "            H = {'tokId': tokId, \n",
    "                 'sntId' : sntId, \n",
    "                 'cur': end+start,\n",
    "                 'tok' : tok, \n",
    "                 'space' :space, \n",
    "                 'pos' : pos\n",
    "                }\n",
    "            # memorize (tok, tokId)\n",
    "            s[i] = (tok, tokId)\n",
    "            \n",
    "            L.append(H)\n",
    "#            print(f\"id:{tokId} cur:{cur}\\t{tok:<20}\\tend0:{end} space:{start}>{space}< {pos}\")\n",
    "    \n",
    "            end += start + len(tok) \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bde91c-f25d-4114-bf6b-7ca2ffe5a238",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d66781-1952-4148-b0ec-d022f1039e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"麩菓子は、\n",
    "麩を主材料とした日本の菓子。\n",
    "\n",
    "麩菓子は、麩を主材料とした日本の菓子。麩菓子は、麩を主材料とした日本の菓子。\n",
    "\n",
    "麩菓子は、麩を主材料とした日本の菓子。\"\"\"\n",
    "\n",
    "RUC17T = \"\"\"  杀人犯护士受到了四次终身监禁处罚\n",
    "今日，医院护士柯林·诺瑞思被投入监狱终身监禁，因为他杀死了四位病人。2002年，来自格拉斯哥的32岁的诺瑞思通过给病人服用大量的安眠药杀死了四位女病人。昨日，经过一段长时间的审判，诺瑞思被指控犯有四起谋杀案。对于每起谋杀案，他至少需要服刑三十年。警方人员克里斯·格雷格谈到，此前诺瑞思一直在医院附近行动诡异。直到一些其他医院工作人员发现时才停止了他的谋杀行为。警方获悉，诺瑞思杀人的动机是不喜欢同老年人一起工作。此案所有受害者均是年老患有心脏病的妇女。这些人都是医院工作人员的负担。\n",
    "\"\"\"\n",
    "\n",
    "segmentText(RUC17T, 'zh', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d8dff-d636-4529-8aa3-ce1d72aacb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2f80c-500e-485e-92f7-343269313217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "# Initialize Tagger for \"wakati\" (word segmentation) output\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "print(wakati.parse(\"pythonが大好きです\").split())\n",
    "\n",
    "# Initialize Tagger for detailed morphological analysis\n",
    "tagger = MeCab.Tagger()\n",
    "print(tagger.parse(\"これはペンです\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd38b2-e1d3-4c50-8950-72270d4b8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example for merging \n",
    "L = [\n",
    " {'src': 2, 'tgt': 2},\n",
    " {'src': 3, 'tgt': 2},\n",
    " {'src': 2, 'tgt': 3},\n",
    " {'src': 3, 'tgt': 3},\n",
    " {'src': 2, 'tgt': 4},\n",
    " {'src': 3, 'tgt': 4},\n",
    "]\n",
    "\n",
    "\n",
    "merge_alignments_graph(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde8383-9a81-4fc1-8156-b2cf47224868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "m = MeCab.Tagger('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef468-00e6-4d77-b917-7c709d4c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "# test whether texts are identical\n",
    "\n",
    "# Split the strings into lists of lines\n",
    "lines1 = text1.splitlines()\n",
    "lines2 = text2.splitlines()\n",
    "\n",
    "# Use ndiff to find the differences\n",
    "diff_result = difflib.ndiff(lines1, lines2)\n",
    "\n",
    "# Print the differences\n",
    "for line in diff_result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6b36e-ed47-4933-8853-529604cbbe46",
   "metadata": {},
   "source": [
    "## Segment / Word Alignment\n",
    "- segment: sentence by sentence\n",
    "- word alignment\n",
    "- merged groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d498d4-c3ff-4a5f-a12f-7ac1ff10b3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# build alignment segments with m *n sentence alignment groups \n",
    "def snt2segAlign(STnt, FTnt, SntAlign):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert sentence alignments to token-level aligned segments.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    STnt : list of lists of Source Text (tokens , tokIds)\n",
    "        Source sentences: [[(token, id), (token, id), ...], ...]\n",
    "        Example: [[('Killer', 1), ('nurse', 2), ...], ...]\n",
    "    \n",
    "    FTnt : list of lists of Target Text (tokens , tokIds)\n",
    "        Target sentences: [[(token, id), (token, id), ...], ...]\n",
    "        Example: [[('El', 1), ('enfermero', 2), ...], ...]\n",
    "    \n",
    "    SntAlign : list of aligned sentence ids per segment (list of dicts)\n",
    "        Sentence alignments: [{'src': '1', 'tgt': '1'}, {'src': '2+3', 'tgt': '2+3'}, ...]\n",
    "        where: \n",
    "        'src': source sentence id\n",
    "        'tgt': target sentence id\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SEGS : dict of alignment segments\n",
    "        Alignment groups with tokens and IDs\n",
    "    \"\"\"\n",
    "    \n",
    "    s = len(STnt)\n",
    "    t = len(FTnt)\n",
    "\n",
    "    # list of tokens per sentence\n",
    "    STok = [[t for t, i in s] for s in STnt]\n",
    "    FTok = [[t for t, i in s] for s in FTnt]\n",
    "    # list of token ids per sentence\n",
    "    STid = [[i for t, i in s] for s in STnt]\n",
    "    FTid = [[i for t, i in s] for s in FTnt]\n",
    "\n",
    "    SEGS = {}\n",
    "    ag = 0\n",
    "    # loop over aligned sentence ids\n",
    "    for aln in SntAlign:\n",
    "        # ST sentences \n",
    "        sIds = [int(s)-1 for s in aln['src'].split('+')]\n",
    "        # aligned TT sentences\n",
    "        tIds = [int(s)-1 for s in aln['tgt'].split('+')]\n",
    "        \n",
    "        SEGS.setdefault(ag, {})\n",
    "        SEGS[ag]['src'] = []\n",
    "        SEGS[ag]['tgt'] = []\n",
    "        SEGS[ag]['sid'] = []\n",
    "        SEGS[ag]['tid'] = []\n",
    "\n",
    "        # join ST/TT tokens of a segment\n",
    "        for i in sIds : SEGS[ag]['src'].extend(STok[i])\n",
    "        for i in tIds : SEGS[ag]['tgt'].extend(FTok[i])\n",
    "        # join ST/TT token ids of a segment\n",
    "        for i in sIds : SEGS[ag]['sid'].extend(STid[i])\n",
    "        for i in tIds : SEGS[ag]['tid'].extend(FTid[i])        \n",
    "        ag += 1\n",
    "        \n",
    "    return SEGS\n",
    "\n",
    "# random word alignmet per bilingual segment \n",
    "def rndAlignment(SEGS):\n",
    "\n",
    "    # random word alignment\n",
    "    L = []\n",
    "    for ag in SEGS:\n",
    "        SEGS[ag]['aln'] = []\n",
    "\n",
    "        for i in range(int((len(SEGS[ag]['sid']) / 1.5))) :\n",
    "            # Get a random index from the list\n",
    "            rs = random.randint(0, len(SEGS[ag]['sid']) - 1)\n",
    "            rt = random.randint(0, len(SEGS[ag]['tid']) - 1)\n",
    "\n",
    "            L.append({'src' : SEGS[ag]['sid'][rs], 'tgt':SEGS[ag]['tid'][rt]})\n",
    "\n",
    "    M = merge_alignments_graph(L)\n",
    "                \n",
    "    return M\n",
    "\n",
    "# simAlign word alignmet per bilingual segment \n",
    "def simAlignment(SEGS):\n",
    "    \n",
    "    aln = []\n",
    "    for seg in SEGS :\n",
    "        # needs to be initialized globally\n",
    "        # returns a dictionary of aligned indexes {key: [(s,t), (s,t), ...]}\n",
    "        A = myaligner.get_word_aligns(SEGS[seg]['src'], SEGS[seg]['tgt'])\n",
    "\n",
    "        # map simalign segment-relative indexes into TPR-DB text-relative indexes\n",
    "        for m in A:\n",
    "            for s, t in A[m]:\n",
    "                aln.append({'src' : SEGS[seg]['sid'][s], 'tgt': SEGS[seg]['tid'][t]})\n",
    "                \n",
    "    return merge_alignments_graph(aln)\n",
    "                \n",
    "\n",
    "def sntAlignment(STnt, FTnt):\n",
    "    s = len(STnt)\n",
    "    t = len(FTnt)\n",
    "    \n",
    "    L = []\n",
    "    for i in range(min(s, t)):\n",
    "        L.append({'src': i+1, 'tgt': i+1})\n",
    "    \n",
    "    if(s > t) :\n",
    "        for i in range(t, s): \n",
    "            L.append({'src': i+1, 'tgt': t})\n",
    "        \n",
    "    if(t > s) :\n",
    "        for i in range(s, t): \n",
    "            L.append({'src': s, 'tgt': i+1})\n",
    "\n",
    "    # bring into a grouped format\n",
    "    M = merge_alignments_graph(L) \n",
    "    return M\n",
    "\n",
    "\n",
    "def merge_alignments_graph(alignments):\n",
    "    \"\"\"\n",
    "    Use graph-based approach to find connected components.\n",
    "    Alignments that share src or tgt indices are in the same group.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not alignments:\n",
    "        return []\n",
    "    \n",
    "    # Build graph of connections\n",
    "    graph = defaultdict(set)\n",
    "    \n",
    "    for i, align in enumerate(alignments):\n",
    "        graph[i].add(i)\n",
    "    \n",
    "    # Connect alignments that share indices\n",
    "    for i in range(len(alignments)):\n",
    "        for j in range(i + 1, len(alignments)):\n",
    "            if (alignments[i]['src'] == alignments[j]['src'] or\n",
    "                alignments[i]['tgt'] == alignments[j]['tgt']):\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)\n",
    "    \n",
    "    # Find connected components\n",
    "    visited = set()\n",
    "    components = []\n",
    "    \n",
    "    def dfs(node, component):\n",
    "        if node in visited:\n",
    "            return\n",
    "        visited.add(node)\n",
    "        component.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            dfs(neighbor, component)\n",
    "    \n",
    "    for i in range(len(alignments)):\n",
    "        if i not in visited:\n",
    "            component = set()\n",
    "            dfs(i, component)\n",
    "            components.append(component)\n",
    "    \n",
    "    # Build merged results\n",
    "    merged = []\n",
    "    for component in components:\n",
    "        src_indices = set()\n",
    "        tgt_indices = set()\n",
    "        for idx in component:\n",
    "            src_indices.add(alignments[idx]['src'])\n",
    "            tgt_indices.add(alignments[idx]['tgt'])\n",
    "        \n",
    "        merged.append({\n",
    "            'src': sorted(src_indices),\n",
    "            'tgt': sorted(tgt_indices)\n",
    "        })\n",
    "    \n",
    "    # Sort by first src index\n",
    "    merged.sort(key=lambda x: int(x['src'][0]))\n",
    "\n",
    "    M = []\n",
    "    for item in merged:\n",
    "        src_str = '+'.join(map(str, item['src'])) if len(item['src']) > 1 else str(item['src'][0])\n",
    "        tgt_str = '+'.join(map(str, item['tgt'])) if len(item['tgt']) > 1 else str(item['tgt'][0])\n",
    "        M.append({'src': src_str, 'tgt': tgt_str})\n",
    "    \n",
    "    return M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f95932-32ad-4bc1-a48e-9a8eea5f4054",
   "metadata": {},
   "source": [
    "## Keystroke mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e40e5e-1bef-4df5-b7a5-160a7fb4b63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"/data/critt/tprdb/TPRDB\"\n",
    "studies = ['BML12', 'SG12', 'RUC17']\n",
    "studies = ['BML12','SG12']\n",
    "studies = ['AR22']\n",
    "studies = ['RUC17']\n",
    "studies = ['SG12']\n",
    "studies = ['NJ12']\n",
    "\n",
    "for study in studies:\n",
    "    files = glob.glob(f\"{path}/{study}/Translog-II/*.xml\")\n",
    "    if(verbose): print(f\"Reading:{study}\\twith {len(files)} files\")\n",
    "\n",
    "    for fn in sorted(files):\n",
    "        base = os.path.basename(fn)\n",
    "        out = f\"TESTED/{study}-{base}\"\n",
    "        print(f\"{fn} --> {out}\")\n",
    "        if os.path.exists(out):\n",
    "            if(verbose): print(f\"  \\tExists: {out}\")\n",
    "            continue\n",
    "\n",
    "        Modifs = OnlyKeyMapping(fn, verbose=0, stanzaFeats=0) \n",
    "        with open(out, \"w\") as f:\n",
    "            print(Modifs, file=f)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e0285-97d8-4cf5-86c5-cc36838b56ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P01_T2.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P04_T2.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P03_T5.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P03_T6.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P05_T3.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P06_T6.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P09_T5.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P12_T3.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P13_T5.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P28_T6.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/BML12/Translog-II/P29_T5.xml'\n",
    "\n",
    "fn = '/data/critt/tprdb/TPRDB/SG12/Translog-II/P01_T1.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/SG12/Translog-II/P02_T3.xml'\n",
    "fn = '/data/critt/tprdb/TPRDB/NJ12/Translog-II/P01_P3.xml'\n",
    "\n",
    "\n",
    "### Root of the WorkSpace XML output file\n",
    "WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "\n",
    "### Read Translog-XML file\n",
    "translog = ET.parse(fn)\n",
    "# Root of the Translog XML input file\n",
    "translog_root = translog.getroot()\n",
    "\n",
    "# get Source and Target Languages\n",
    "e = translog_root.find('.//Languages')\n",
    "SL = e.get('source') \n",
    "TL = e.get('target')\n",
    "\n",
    "# get final text from Translog file \n",
    "FText = getFinalText(translog_root)\n",
    "\n",
    "# segment and tokenize the target text\n",
    "(FTsnt, FToken, FToken_root) = Tokenize(FText, TL, 'FinalToken')\n",
    "\n",
    "Keys = KeyMapping(FText, FToken, translog_root, Verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dfd19-f2b9-410f-8dbe-50fc752c1577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Keys = KeyMapping(FText, FToken, translog_root, Verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c72a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlyKeyMapping(fn, verbose=0, stanzaFeats=0) :\n",
    "    wks_root = Translog2WKS(fn)\n",
    "    \n",
    "    ### Root of the WorkSpace XML output file\n",
    "    WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "    \n",
    "    ### Read Translog-XML file\n",
    "    translog = ET.parse(fn)\n",
    "    # Root of the Translog XML input file\n",
    "    translog_root = translog.getroot()\n",
    "    \n",
    "    # get Source and Target Languages\n",
    "    e = translog_root.find('.//Languages')\n",
    "    SL = e.get('source') \n",
    "    TL = e.get('target')\n",
    "    \n",
    "    # get final text from Translog file \n",
    "    FText = getFinalText(translog_root)\n",
    "    \n",
    "    # segment and tokenize the target text\n",
    "    (FTsnt, FToken, FToken_root) = Tokenize(FText, TL, 'FinalToken', stanzaFeats=0)\n",
    "    \n",
    "    Keys = KeyMapping(FText, FToken, translog_root, Verbose = 0)\n",
    "\n",
    "    return Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44db4d-0a3d-4a26-9b51-336d13a2524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d793c9-0e93-465d-bcfa-e032e5c5711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyMapping(Text, Token, translog_root, Verbose = 1):\n",
    "\n",
    "    text = list(Text)  # list of characers for final text \n",
    "    index = [0] * len(text) # list of word indexes for final text \n",
    "\n",
    "    pm = 10 # margin for character visualization, left and right\n",
    "\n",
    "    # test whether tokens fit text, print warning\n",
    "    for tok in Token:\n",
    "        cur =  tok['cur']\n",
    "        if(not Text.startswith(tok['tok'],  cur)):\n",
    "            print(f\"\\tWARNING cur:{tok['cur']} token:{tok['tokId']}:>{tok['tok']}< ~~ {''.join(text[max(cur-pm, 0): cur]).replace('\\n', '\\\\n')}>{text[cur]}<{''.join(text[cur+1: min(cur+pm,len(text))]).replace('\\n', '\\\\n')}\")      \n",
    "   \n",
    "    # character arrays of final text\n",
    "    index[0] = 1\n",
    "    for tok in Token:\n",
    "        cur = tok['cur']\n",
    "        end = cur + len(tok['tok'])\n",
    "        \n",
    "        # space before is part of the following token \n",
    "        if('space' in tok) : cur -= len(tok['space'])\n",
    "        index[cur:end] = [tok['tokId']]*(end-cur)\n",
    "\n",
    "    for i in index: \n",
    "        if(index[i] == 0): index[i] = index[i-1]\n",
    "\n",
    "    # find keystrokes in the xml file\n",
    "    Keys = {}\n",
    "    e = translog_root.find('.//Events')\n",
    "    for key in e.findall('Key'):\n",
    "        d = dict(key.attrib)\n",
    "        d['Cursor'] = int(d['Cursor'])\n",
    "        d['Time'] = int(d['Time'])\n",
    "        \n",
    "        time = d['Time']\n",
    "        # two keystrokes at the same time\n",
    "        if(time in Keys) :\n",
    "            print(f\"\\tKeystrokes same time {time}\\t{d['Type']}:{d['Value']} -- {Keys[d[\"Time\"]]['Type']}:{Keys[d[\"Time\"]]['Value']}\")\n",
    "            time +=1\n",
    "        Keys[time] = d\n",
    "\n",
    "    def DeleteString(cur, cut):\n",
    "        cutId = index[-1]\n",
    "        Keys[time]['tokId'] = index[-1]\n",
    "        if(cur < len(index)) : \n",
    "            cutId = index[cur]      \n",
    "            Keys[time]['tokId'] = index[cur]\n",
    "            text[cur:cur] = cut\n",
    "            index[cur:cur] = [cutId] * len(cut)\n",
    "        else :      \n",
    "            text.extend(cut)\n",
    "            index.extend([cutId] * len(cut))\n",
    "\n",
    "    def InsertString(cur, ins):\n",
    "        w = ''\n",
    "        Keys[time]['tokId'] = index[cur]\n",
    "\n",
    "        # insert array of pasted characters\n",
    "        for i in range(len(ins)) :\n",
    "            if(cur >= len(text)):\n",
    "                w = f\"\\tWARNING InsertString: {cur} length text {len(text)}\"\n",
    "            elif((ins[i] != text[cur]) and  (text[cur] != '#') and  (text[cur] != '')):\n",
    "                w = f\"\\tWARNING InsertString: {cur} key:>{ins[i]}< text:>{text[cur]}<\\t\\t{''.join(text[max(cur-pm, 0): cur])}>{text[cur]}<{''.join(text[cur+1: min(cur+pm,len(text))]).replace('\\n', '\\\\n')}\"\n",
    "                \n",
    "            del text[cur]\n",
    "            del index[cur]\n",
    "        return w\n",
    "\n",
    "    Warn = 0\n",
    "    \n",
    "    ############################################################\n",
    "    # main loop over keystrokes in reversed time\n",
    "    for time in  sorted(Keys.keys(), reverse=True) :\n",
    "        tpe = Keys[time]['Type']    # one of 'insert', 'delete', 'edit'\n",
    "\n",
    "        \n",
    "        # navigation not interesting for modifications\n",
    "        if(tpe == 'navi') : continue\n",
    "    \n",
    "        key = Keys[time]['Value']   # the value of the keystroke\n",
    "        cur = Keys[time]['Cursor']  # cursor offeset in text\n",
    "        \n",
    "        # text that is marked and deleted\n",
    "        cut = ''\n",
    "        if('Text' in Keys[time]) : cut = Keys[time]['Text']\n",
    "        if('Paste' in Keys[time]) : cut = Keys[time]['Paste']\n",
    "    \n",
    "        warning = ''\n",
    "\n",
    "        # initialize tokId\n",
    "        if(len(index) > 0) :\n",
    "            if(cur >= len(index)) : Keys[time]['tokId'] = index[-1]\n",
    "            else : Keys[time]['tokId'] = index[cur]\n",
    "        else : Keys[time]['tokId'] = 1       \n",
    "\n",
    "        # plot text changes\n",
    "        if(Verbose) :\n",
    "            print(f\"{time}\\t{tpe} i:{key.replace('\\n', '\\\\n')}< d:{cut.replace('\\n', '\\\\n')}< \\tc:{cur}\\t{''.join(text[max(cur-pm, 0):min(cur+pm,len(text))]).replace('\\n', '\\\\n')}\", end='')\n",
    "\n",
    "        #################################################################\n",
    "        # [Ctrl+V] and [Ctrl+X]\n",
    "        if(tpe == 'edit') :\n",
    "\n",
    "            # insertion: [Ctrl+V]\n",
    "            if('Paste' in Keys[time]) : \n",
    "                ins = list(Keys[time]['Paste'])\n",
    "#                print(\"No paste\")\n",
    "                warning = InsertString(cur, ins)\n",
    "                            \n",
    "            # deletion: [Ctrl+X]\n",
    "            if('Text' in Keys[time]) : \n",
    "                cut = (list(Keys[time]['Text']))\n",
    "                DeleteString(cur+1, cut)\n",
    "\n",
    "        #################################################################\n",
    "        if(tpe == 'insert' or tpe == 'return') :\n",
    "            cut = ''\n",
    "\n",
    "            # marked text that is deleted with the insertion\n",
    "            if('Text' in Keys[time]) : \n",
    "                cut = (list(Keys[time]['Text']))\n",
    "                DeleteString(cur+1, cut)\n",
    "\n",
    "            # insert keystroke\n",
    "            if(tpe == 'return') : key =\"\\n\"\n",
    "            if(key == '' and cut == '') : \n",
    "                key = ' '\n",
    "                print(f\"\\tInsert with no value: {cur}\")\n",
    "            # insert the \n",
    "            warning = InsertString(cur, list(key))\n",
    "    \n",
    "        #################################################################\n",
    "        if(tpe == 'delete') :\n",
    "            if('Text' in Keys[time]) : cut = (list(Keys[time]['Text']))\n",
    "            else: cut = list('#')\n",
    "\n",
    "            # could be: [Back], [Ctrl+Back], [Shift+Back]\n",
    "            if('Back' in key) :\n",
    "                cutId = 1\n",
    "                if(cur >= len(index)) :\n",
    "                    if(len(index) > 0) : cutId = index[-1]                \n",
    "                    Keys[time]['tokId'] = cutId\n",
    "                    text.extend(cut)\n",
    "                    index.extend([cutId] * len(cut))\n",
    "\n",
    "                elif(cur > 0) : \n",
    "#                    print(f\"\\n\\tAAA: {cur} >{cut}< {len(index)} {len(text)}\")\n",
    "                    cutId = index[cur-1]                \n",
    "                    Keys[time]['tokId'] = index[cur-1]\n",
    "                    text[cur:cur] = cut\n",
    "                    index[cur:cur] = [cutId] * len(cut)\n",
    "\n",
    "                # cursor at first position. index / text might be empty\n",
    "                else : \n",
    "                    Keys[time]['tokId'] = 1 \n",
    "                    \n",
    "                    # insert deletion in the first position\n",
    "                    text = cut + text\n",
    "                    index = [cutId] * len(cut) + index\n",
    "\n",
    "            elif('Delete' in key) : DeleteString(cur, cut)\n",
    "            else: \n",
    "                print(f\"Delete key not covered: {key}\")\n",
    "    \n",
    "        if(Verbose) :\n",
    "            print(f\"\\t\\t{''.join(text[max(cur-pm, 0): cur]).replace('\\n', '\\\\n')}@{''.join(text[cur: min(cur+pm,len(text))]).replace('\\n', '\\\\n')}\\tToken:{Keys[time]['tokId']}\\tlen:{len(text)}-{len(index)}\")\n",
    "            if(warning): print(warning)\n",
    "                \n",
    "        if(len(warning) > 1) : Warn += 1\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"\\tRemaining Text length: {len(index)}, Warnings:{Warn}\")\n",
    "    if(Verbose >2) :\n",
    "        print(f\"Remaining Text: {''.join(text)}\")\n",
    "        print(f\"Remaining Text:{index}\")\n",
    "\n",
    "    return Keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030534f-27b1-435b-9e02-dbf52caf5df5",
   "metadata": {},
   "source": [
    "## XML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c4955-f968-4f6a-9084-cc2f4e4e988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert list of dictionary into xml\n",
    "def list_of_dicts_to_xml(data_list, root_tag='root', item_tag='item'):\n",
    "    \"\"\"\n",
    "    Converts a list of dictionaries into an XML root,\n",
    "    placing dictionary values into attributes of XML elements.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): A list of dictionaries to convert.\n",
    "        root_tag (str): The tag name for the root element of the XML.\n",
    "        item_tag (str): The tag name for each item element in the XML.\n",
    "\n",
    "    Returns:\n",
    "        root: The XML root.\n",
    "    \"\"\"\n",
    "    root = ET.Element(root_tag)\n",
    "    for item_dict in data_list:\n",
    "        item_element = ET.SubElement(root, item_tag)\n",
    "        for key, value in item_dict.items():\n",
    "            # Convert value to string as XML attributes are strings\n",
    "            item_element.set(key, str(value))\n",
    "\n",
    "    return root\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621edb4-1e11-48cd-97a4-6099388f3c9c",
   "metadata": {},
   "source": [
    "### Events XML -> WKS XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520f671-7d39-4e04-982f-07f34c2de502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Events Token container -> list of token dictionaries\n",
    "def tokens2dict(xml_root):\n",
    "    \"\"\"Convert list token XML to dictionary.\"\"\"\n",
    "    \n",
    "    tokens = []    \n",
    "    token_dict = {}\n",
    "    for token in xml_root.findall('Token'):\n",
    "        token_old = token_dict\n",
    "        token_dict = dict(token.attrib)\n",
    "\n",
    "        # Convert numeric strings to numbers if needed\n",
    "        if 'id' in token_dict:\n",
    "            token_dict['tokId'] = int(token_dict['id'])\n",
    "\n",
    "        if ('cur' not in token_dict):\n",
    "            print(f\"tokens2dict Warning: no cur snt:{token_dict['segId']} tokId:{token_dict['id']} >{token_dict['tok']}<\")\n",
    "            if('cur' in token_old) : token_dict['cur'] = int(token_old['cur'])\n",
    "            else : token_dict['cur'] = 0\n",
    "        tokens.append(token_dict)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2snt(tokList):\n",
    "    \"\"\"\n",
    "    Convert list of token dictionary to list of lists of sentence.\n",
    "    Assumes tokens have 'segId'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group tokens by sentence\n",
    "    S = []\n",
    "    L = []\n",
    "    segId = 0\n",
    "    for token in tokList:\n",
    "        if(token.get('segId') != segId) :\n",
    "            if(segId != 0) :\n",
    "                S.append(L)\n",
    "                L=[]\n",
    "            segId = token.get('segId')        \n",
    "        L.append((token.get('tok'), token.get('id')))\n",
    "#        print(token.get('segId'), token.get('tok'), token.get('id'))\n",
    "    \n",
    "    if(L) : S.append(L)\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe8ece-fa86-4067-9245-e1536eaef4ed",
   "metadata": {},
   "source": [
    "# Events to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a3346-850e-4f13-98d1-9ea5193bb650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import io\n",
    "\n",
    "path = \"/data/critt/tprdb/TPRDB\"\n",
    "studies = ['BML12', 'SG12', 'RUC17']\n",
    "studies = ['RUC17']\n",
    "verbose = 2\n",
    "    \n",
    "for study in studies:\n",
    "    files = glob.glob(f\"{path}/{study}/Events/*Event.xml\")\n",
    "    if(verbose): print(f\"Reading:{study}\\twith {len(files)} files\")\n",
    "    try:\n",
    "        os.mkdir(f\"{path}/{study}/WKS/\")\n",
    "    except FileExistsError:\n",
    "        print(f\"\\tDirectory WKS already exists.\")\n",
    "\n",
    "    n = 0\n",
    "    for fn in sorted(files):\n",
    "        root, extension = os.path.splitext(fn)\n",
    "        wks = os.path.basename(root).removesuffix(\".Event\")\n",
    "        out = f\"{path}/{study}/WKS/{wks}.xml\"\n",
    "        n += 1\n",
    "        \n",
    "        if os.path.exists(out):\n",
    "            if(verbose): print(f\"  {n}\\tExists: {out}\")\n",
    "            continue\n",
    "            \n",
    "        if(verbose):  print(f\"  {n}\\tOutput: {out}\")\n",
    "            \n",
    "        try:\n",
    "            WorkSpace_root = Events2WKS(fn) \n",
    "        except FileExistsError:\n",
    "            print(f\"\\tError in XML File.\")\n",
    "            continue\n",
    "        \n",
    "        ET.indent(WorkSpace_root, space='  ')  # 2 spaces\n",
    "        with open(out, \"w\") as f:\n",
    "            print(ET.tostring(WorkSpace_root, encoding='unicode'), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaa6fb-01ab-43de-8c26-bf7897420166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = \"/data/critt/tprdb/TPRDB/ENJA15/Events/P01_T1.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Events/P22_P5.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB//RUC17/Events/P02_P3.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB//RUC17/Events/P02_T1.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB//RUC17/Events/P03_T1.Event.xml\"\n",
    "\n",
    "\n",
    "WorkSpace_root = Events2WKS(fn, Verbose=1) \n",
    "\n",
    "##################################################################\n",
    "# pretty-print WorkSpace_root\n",
    "ET.indent(WorkSpace_root, space='  ')  # 2 spaces\n",
    "print(ET.tostring(WorkSpace_root, encoding='unicode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c2869-34c1-46aa-b5e4-e34f4411e893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "Verbose = 1\n",
    "fn = \"/data/critt/tprdb/TPRDB//RUC17/Events/P02_T1.Event.xml\"\n",
    "\n",
    "### Root of the WorkSpace XML output file\n",
    "WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "\n",
    "if(Verbose) : print(f\"Events2WKS 1\")\n",
    "\n",
    "### Read Events-XML file\n",
    "events = ET.parse(fn)\n",
    "if(Verbose) : print(f\"Events2WKS 2\")\n",
    "# Root of the Translog XML input file\n",
    "events_root = events.getroot()\n",
    "if(Verbose) : print(f\"Events2WKS 2\")\n",
    "\n",
    "# get Source and Target Languages\n",
    "e = events_root.find('.//Languages')\n",
    "SL = e.get('source') \n",
    "TL = e.get('target')\n",
    "\n",
    "if(Verbose) : print(f\"Events2WKS: SL:{SL} TL:{TL}\")\n",
    "    \n",
    "##################################################################\n",
    "e = events_root.find(f\".//SourceToken\")\n",
    "token = tokens2dict(e)\n",
    "# Reconstruct Sentence from Tokens \n",
    "STsnt = tokens2snt(token)\n",
    "\n",
    "e = events_root.find(f\".//FinalToken\")\n",
    "token = tokens2dict(e)\n",
    "# Reconstruct Sentence from Tokens \n",
    "FTsnt = tokens2snt(token)\n",
    "\n",
    "######################################\n",
    "e = events_root.find('.//Salignment')\n",
    "L = []\n",
    "for token in e.findall('Salign'):\n",
    "    d = dict(token.attrib)\n",
    "    L.append(d)\n",
    " \n",
    "SntAlign = merge_alignments_graph(L)\n",
    "\n",
    "segAlign = snt2segAlign(STsnt, FTsnt, SntAlign)\n",
    "#tokAlign = rndAlignment(segAlign)\n",
    "tokAlign = simAlignment(segAlign)\n",
    "\n",
    "\n",
    "rndAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cc7b6-d50e-452d-b73c-94e5e6d70c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "SntAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6a5bd-c047-4edf-817a-8f70b6d23e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Events2WKS(fn, Verbose = 0) :\n",
    "    \n",
    "    ### Root of the WorkSpace XML output file\n",
    "    WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "    \n",
    "    if(Verbose) : print(f\"Events2WKS 1\")\n",
    "    \n",
    "    ### Read Events-XML file\n",
    "    events = ET.parse(fn)\n",
    "    if(Verbose) : print(f\"Events2WKS 2\")\n",
    "    # Root of the Translog XML input file\n",
    "    events_root = events.getroot()\n",
    "    if(Verbose) : print(f\"Events2WKS 2\")\n",
    "    \n",
    "    # get Source and Target Languages\n",
    "    e = events_root.find('.//Languages')\n",
    "    SL = e.get('source') \n",
    "    TL = e.get('target')\n",
    "\n",
    "    if(Verbose) : print(f\"Events2WKS: SL:{SL} TL:{TL}\")\n",
    "        \n",
    "    ##################################################################\n",
    "    \n",
    "    def EventsToken(tag, lng, Verbose=0) :\n",
    "        \n",
    "        e = events_root.find(f\".//{tag}\")\n",
    "        token = tokens2dict(e)\n",
    "        \n",
    "        if(Verbose) : print(f\"EventsToken: lng:{lng} No:{len(token)}\")\n",
    "\n",
    "        # Reconstruct Sentence from Tokens \n",
    "        snt = tokens2snt(token)\n",
    "        \n",
    "        # get additional features from Stanza \n",
    "        # add features to list of STokens \n",
    "        tokFeats = stanzaFeatures(snt, lng, token)\n",
    "        \n",
    "        # convert token Dictionary to xml\n",
    "        token_root = list_of_dicts_to_xml(tokFeats, root_tag=tag, item_tag='Token')\n",
    "        \n",
    "        # assign source language \n",
    "        token_root.set('language', str(lng))\n",
    "        return token_root\n",
    "    \n",
    "    ######################################\n",
    "    # append FT tokenization to WorkSpace root \n",
    "    Root = EventsToken('SourceToken', SL, Verbose=Verbose)\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    # append FT tokenization to WorkSpace root \n",
    "    Root = EventsToken('FinalToken', TL, Verbose=Verbose)\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ######################################\n",
    "    e = events_root.find('.//Salignment')\n",
    "    L = []\n",
    "    for token in e.findall('Salign'):\n",
    "        d = dict(token.attrib)\n",
    "        L.append(d)\n",
    "     \n",
    "    M = merge_alignments_graph(L)\n",
    "    if(Verbose) : print(f\"Salignment: {len(M)}\")\n",
    "    \n",
    "    # map into xml format \n",
    "    Root = list_of_dicts_to_xml(M, root_tag='SntAlignment', item_tag='Snt')\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ######################################\n",
    "    e = events_root.find('.//Alignment')\n",
    "    L = []\n",
    "    for token in e.findall('Align'):\n",
    "        d = dict(token.attrib)\n",
    "        d['src'] = d['sid']\n",
    "        d['tgt'] = d['tid']\n",
    "        L.append(d)\n",
    "     \n",
    "    M = merge_alignments_graph(L)\n",
    "    if(Verbose) : print(f\"Alignment: {len(M)}\")\n",
    "    \n",
    "    # map into xml format \n",
    "    Root = list_of_dicts_to_xml(M, root_tag='TokAlignment', item_tag='Tok')\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ##################################################################\n",
    "    e = events_root.find('.//Modifications')\n",
    "    \n",
    "    L = []\n",
    "    for token in e.findall('Mod'):\n",
    "        d = dict(token.attrib)\n",
    "        # rename sid and tid\n",
    "        d['src'] = d.pop('sid')\n",
    "        d['tgt'] = d.pop('tid')\n",
    "    \n",
    "        L.append(d)\n",
    "\n",
    "    if(Verbose) : print(f\"Modifications: {len(L)}\")\n",
    "\n",
    "    Root = list_of_dicts_to_xml(L, root_tag='Modifications', item_tag='Mod')\n",
    "    WorkSpace_root.append(Root)\n",
    "     \n",
    "    \n",
    "    ##################################################################\n",
    "    e = events_root.find('.//Fixations')\n",
    "    L = []\n",
    "    for token in e.findall('Fix'):\n",
    "        d = dict(token.attrib)\n",
    "        \n",
    "        # rename sid and tid\n",
    "        d['src'] = d.pop('sid')\n",
    "        d['tgt'] = d.pop('tid')\n",
    "        L.append(d)\n",
    "\n",
    "    if(Verbose) : print(f\"Fixations: {len(L)}\")\n",
    "        \n",
    "    Root = list_of_dicts_to_xml(L, root_tag='Fixations', item_tag='Fix')\n",
    "    WorkSpace_root.append(Root)    \n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment open-closing\n",
    "    #  <Segments>\n",
    "    #    <Seg sntId=\"1\" open=\"72952\" close=\"89436\" />\n",
    "    e = events_root.find('.//Segments')\n",
    "    L = []\n",
    "    for token in e.findall('Seg'):\n",
    "        d = dict(token.attrib)\n",
    "        L.append(d)\n",
    "        \n",
    "    if(Verbose) : print(f\"Segments: {len(L)}\")\n",
    "\n",
    "    Root = list_of_dicts_to_xml(L, root_tag='SntEdits', item_tag='Snt')\n",
    "    WorkSpace_root.append(Root)\n",
    "\n",
    "    return WorkSpace_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8633f-60f5-4595-a370-09f4cecc3b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
