{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595310a0-37b3-4968-8fad-547b468c6e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "import stanza\n",
    "import jieba.posseg as pseg\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b77300-dd29-4841-8fea-80fb8c0b1877",
   "metadata": {},
   "source": [
    "# Translog to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f61730-3f37-4b98-b518-b402073d3414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fn = \"/data/critt/tprdb/TPRDB/AR22/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/SG12/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Translog-II/P01_T2.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Translog-II/P01_T1.xml\"\n",
    "#fn = \"/data/critt/tprdb/TPRDB/RUC17/Translog-II/P01_T1.xml\"\n",
    "#fn = \"/data/critt/tprdb/TPRDB/STC17bolt/Translog-II/P01_T1.xml\"\n",
    "#fn = \"/data/critt/tprdb/TPRDB/ENJA15/Translog-II/P01_T1.xml\"\n",
    "\n",
    "wks_root = Translog2WKS(fn)\n",
    "\n",
    "# pretty-print WorkSpace_root\n",
    "ET.indent(wks_root, space='  ')  # 2 spaces\n",
    "print(ET.tostring(wks_root, encoding='unicode'))\n",
    "\n",
    "## to do\n",
    "# Realign and substitute in WKS\n",
    "# Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e65671-ff63-4dc6-98ef-cec20d14d85a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Translog2WKS(fn) :\n",
    "    \n",
    "    ### Root of the WorkSpace XML output file\n",
    "    WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "    \n",
    "    ### Read Translog-XML file\n",
    "    translog = ET.parse(fn)\n",
    "    # Root of the Translog XML input file\n",
    "    translog_root = translog.getroot()\n",
    "    \n",
    "    # get Source and Target Languages\n",
    "    e = translog_root.find('.//Languages')\n",
    "    SL = e.get('source') \n",
    "    TL = e.get('target')\n",
    "    \n",
    "    ##################################################################\n",
    "    ### ST segmentation and tokenization\n",
    "    # <SourceToken language=\"en\" >\n",
    "    #    <Token cur=\"0\" tokId=\"1\" pos=\"NNP\" sntId=\"1\" tok=\"Killer\" />\n",
    "      \n",
    "    # Source text and target Text \n",
    "    SText = getSourceText(translog_root)\n",
    "    \n",
    "    # segment and tokenize the source text\n",
    "    (STsnt, SToken, SToken_root) = Tokenize(SText, SL, 'SourceToken')\n",
    "    \n",
    "    # append ST tokenization to WorkSpace root \n",
    "    WorkSpace_root.append(SToken_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### TT final target text (translation) segmentation and tokenization\n",
    "    #  <FinalToken language=\"ar\" >\n",
    "    #    <Token  cur=\"0\" tokId=\"1\" sntId=\"1\" tok=\"ﺎﻠﻤﻣﺮﺿ\" />  \n",
    "    \n",
    "    # get final text from Translog file \n",
    "    FText = getFinalText(translog_root)\n",
    "    \n",
    "    # segment and tokenize the target text\n",
    "    (FTsnt, FToken, FToken_root) = Tokenize(FText, TL, 'FinalToken')\n",
    "    \n",
    "    # append FT tokenization to WorkSpace root \n",
    "    WorkSpace_root.append(FToken_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment-alignment\n",
    "    #  <SntAlign>\n",
    "    #     <Snt src=\"1\" tgt=\"1\" />\n",
    "    \n",
    "    # very preliminary Sentence Alignment\n",
    "    SntAlignList = sntAignment(STsnt, FTsnt)\n",
    "    \n",
    "    # convert SntAlign Dictionary to xml\n",
    "    SntAln_root = list_of_dicts_to_xml(SntAlignList, root_tag='SntAlign', item_tag='Snt')\n",
    "    \n",
    "    # append SntAlign Dictionary to WorkSpace root \n",
    "    WorkSpace_root.append(SntAln_root)\n",
    "    \n",
    "    ##################################################################\n",
    "    ### Token-alignment\n",
    "    #  <TokAlign>\n",
    "    #     <Tok sid=\"1\" tid=\"1\" />\n",
    "\n",
    "    tokAlignList = tokAlignment(STsnt, FTsnt, SntAlignList)\n",
    "    \n",
    "    # convert SntAlign Dictionary to xml\n",
    "    TokAln_root = list_of_dicts_to_xml(tokAlignList, root_tag='TokAlign', item_tag='Tok')\n",
    "    \n",
    "    # append SntAlign Dictionary to WorkSpace root \n",
    "    WorkSpace_root.append(TokAln_root)\n",
    "\n",
    "    ##################################################################\n",
    "    ### Keystroke-Token mapping\n",
    "    #  <Modifications>\n",
    "    #    <Mod time=\"72953\" type=\"Mins\" cur=\"0\" chr=\"ﺍ\" X=\"0\" Y=\"0\" sntId=\"1\" sid=\"2\" tid=\"1\"  />\n",
    "     \n",
    "    ##################################################################\n",
    "    ### Fixation-Token mapping\n",
    "    #  <Fixations>\n",
    "    #      <Fix time=\"30\" win=\"1\" cur=\"227\" dur=\"175\" X=\"502\" Y=\"228\" sntId=\"3\" sid=\"41\" tid=\"39\" />\n",
    "    \n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment open-closing\n",
    "    #  <Segments>\n",
    "    #    <Seg sntId=\"1\" open=\"72952\" close=\"89436\" />\n",
    "\n",
    "    return WorkSpace_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ec3d0-3af4-4df6-bbe4-9d4cad3d655a",
   "metadata": {},
   "source": [
    "## Linguistic processing\n",
    "- sentence segmentation (NLTK)\n",
    "- tokenization (NLTK)\n",
    "- lexical features (Stanza)\n",
    "- cursor offset of words in text\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b55e-049d-413e-b554-9507a7c03240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(text, lng, tag):\n",
    "    \"\"\"\n",
    "    Tokenize and annotate text with linguistic features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw input text to process\n",
    "    lng : str\n",
    "        Language code (e.g., 'en', 'es', 'de')\n",
    "    tag : str\n",
    "        XML root tag name (e.g., 'SourceText', 'TargetText')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (snt, toksFeats, token_root)\n",
    "        - snt: List of tokenized sentences with POS tags\n",
    "        - toksFeats: List of token dictionaries with all features\n",
    "        - token_root: XML ElementTree with all token data\n",
    "    \"\"\"\n",
    "    \n",
    "    # segment and tokenize source text \n",
    "    # snt: is list of tokenized ,Tagged sentences: \n",
    "    #    [[(token, pos), ...], [(token, pos), ... ], ...]  \n",
    "    snt = segmentText(text, lng=lng, flag=1)\n",
    "    \n",
    "    # create list of tokens with sntId, tokId, cursor offset\n",
    "    #    [{tok1features}, {tok2features}, ...]\n",
    "    toksList = tokenCurOffset(text, snt)\n",
    "\n",
    "    # get additional features from Stanza \n",
    "    # add features to list of STokens \n",
    "    toksFeats = stanzaFeatures(snt, lng, toksList)\n",
    "    \n",
    "    # convert token Dictionary to xml\n",
    "    token_root = list_of_dicts_to_xml(toksFeats, root_tag=tag, item_tag='Token')\n",
    "    \n",
    "    # assign source language \n",
    "    token_root.set('language', str(lng))\n",
    "\n",
    "    return (snt, toksFeats, token_root)\n",
    "\n",
    "\n",
    "# get ST from the Translog file\n",
    "def getSourceText(root):\n",
    "    \n",
    "    # get text from UTF8 container in the xml file \n",
    "    ST = root.find('.//SourceTextUTF8')\n",
    "    if ST is not None:\n",
    "        return ST.text\n",
    "        \n",
    "    # in older versions there is no UTF8 version in the xml file \n",
    "    # else SourceTextChar must extist\n",
    "    text2 = ''\n",
    "    STchars = root.findall('.//SourceTextChar/CharPos')\n",
    "    for chars in STchars:\n",
    "        text2 += chars.get('Value')\n",
    "    return text2\n",
    "\n",
    "# get FT from the Translog file\n",
    "def getFinalText(root):\n",
    "\n",
    "    # FinalText in UTF8 should usually always be there\n",
    "    FT = root.find('.//FinalText')\n",
    "    if FT is not None:\n",
    "        return FT.text\n",
    "\n",
    "    # else FinalTextChar must extist\n",
    "    text2 = ''\n",
    "    FTchars = root.findall('.//FinalTextChar/CharPos')\n",
    "    for chars in FTchars:\n",
    "        text2 += chars.get('Value')\n",
    "    return text2\n",
    "\n",
    "# segment text FT from the Translog file\n",
    "def segmentText(text, lng='en', flag = 1):\n",
    "    \n",
    "    # replace multiple \\n by one (no impact on NLTK segmentation)\n",
    "    text1 = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    if(lng == 'zh') : return segmentChinese(text, flag)\n",
    "    if(lng == 'ja') : return segmentChinese(text, flag)\n",
    "        \n",
    "    # Segment text into list of sentences\n",
    "    snt0 = sent_tokenize(text1)\n",
    "\n",
    "    # segment text at newline into segments (not covered by NLTK)\n",
    "    snt1 = []\n",
    "    for i in range(len(snt0)) :\n",
    "        s = snt0[i]\n",
    "        snt1.extend(s.split('\\n'))\n",
    "\n",
    "    # Tokenize each sentences\n",
    "    snt1 = [word_tokenize(s) for s in snt1]\n",
    "    \n",
    "    # Part-of-speech tagging each sentences: works only properly for English\n",
    "    if(flag == 1) : snt1 = [pos_tag(s) for s in snt1]\n",
    "\n",
    "    # collapse back into list of sentences\n",
    "    if(flag == 2) : snt1 = [\" \".join(s) for s in snt1]\n",
    "    return snt1\n",
    "\n",
    "def segmentChinese(T, flag) :\n",
    "    pattern = r'[。！？]'\n",
    "    \n",
    "    words = pseg.cut(T)\n",
    "#    print(T, '\\n')\n",
    "    S = []\n",
    "    L = []\n",
    "    for tok, pos in words:\n",
    "        # skip word that are whitespaces\n",
    "        if(re.search(r'\\s',  tok)) : \n",
    "            # new sentence with \\n,but it's not a token\n",
    "            if(re.search(r'\\n',  tok)) : \n",
    "                S.append(L)\n",
    "                L = []\n",
    "            continue\n",
    "            \n",
    "        # token and pos\n",
    "        if(flag == 1) : L.append((tok, pos))\n",
    "        # only token if flag != 1\n",
    "        else : L.append(tok)\n",
    "\n",
    "#        print(tok, pos)\n",
    "        # end of sentence\n",
    "        match = re.search(pattern, tok)\n",
    "        if(match) : \n",
    "            S.append(L)\n",
    "            L = []\n",
    "\n",
    "    return S\n",
    " \n",
    "# additional features from Stanza\n",
    "def stanzaFeatures(sntList, lng, tokList, processor='tokenize,mwt,pos,lemma', tokenize_no_ssplit=False, verbose=False):\n",
    "\n",
    "    # mwt not available for those languages\n",
    "    if (lng == 'ja' or lng == 'zh' or lng == 'ko' or lng == 'nl'): processor = 'tokenize,pos,lemma'\n",
    "\n",
    "    # initialize stanza pipeline\n",
    "    nlp = stanza.Pipeline(lang=lng, processors=processor, tokenize_pretokenized=True, tokenize_no_ssplit=tokenize_no_ssplit, verbose=verbose)\n",
    "    \n",
    "    # take out the POS tags from the list of sentences\n",
    "    sntList = [[w for w, p in s] for s in sntList]\n",
    "    doc = nlp(sntList)\n",
    "\n",
    "    # stanza document to list of list of dictionaries\n",
    "    stza_list = doc.to_dict()\n",
    "    \n",
    "    # map list of NLTK tokens into dictinary for faster lookup \n",
    "    TD = {d['tokId']: d for d in tokList}\n",
    "\n",
    "    sntId = 0\n",
    "    tokId = 0\n",
    "    Token = []\n",
    "    off = 0\n",
    "\n",
    "    for snt in stza_list:\n",
    "        sntId +=1\n",
    "        for tok in snt :\n",
    "            tokId +=1\n",
    "            tok['sntId'] = sntId\n",
    "            tok['tokId'] = tokId\n",
    "            \n",
    "            # these features must be identical\n",
    "            if(tok['text'] != TD[tokId]['tok']) :\n",
    "                print(f\"stanzaFeatures Error: snt:{sntId} tokId:{tokId} stanzaWord:{tok['text']} NLTKWord:{TD[tokId]['tok']}\")\n",
    "                \n",
    "            # rename into tok\n",
    "            tok['tok'] = tok.pop('text')\n",
    "            \n",
    "            # copy from tokList\n",
    "            tok['cur'] = TD[tokId]['cur']\n",
    "            \n",
    "            if('space' in TD[tokId]):  tok['space'] = TD[tokId]['space']\n",
    "            else:  tok['space'] = ''\n",
    "\n",
    "            # pos tag from NLTK\n",
    "            if('pos' in TD[tokId]): tok['pos'] = TD[tokId]['pos']\n",
    "\n",
    "            # delete Stanza features\n",
    "            if('misc' in tok): tok.pop(\"misc\")\n",
    "            if('id' in tok): tok.pop(\"id\")\n",
    "            if('start_char' in tok): tok.pop(\"start_char\")\n",
    "            if('end_char' in tok): tok.pop(\"end_char\")\n",
    "            Token.append(tok)\n",
    "    return Token\n",
    "\n",
    "\n",
    "# Find cursor offset for tokens in text\n",
    "def tokenCurOffset(text, sntList): \n",
    "    \n",
    "    L = [] # list of dictionaries the contain Token information\n",
    "    end = 0 # position of end of previous word in text\n",
    "    tokId = 0  # word ID\n",
    "    sntId = 0  # sentence ID\n",
    "\n",
    "    for snt in sntList:\n",
    "        sntId += 1\n",
    "        #for tok, pos in snt:\n",
    "        for i in range(len(snt)):\n",
    "            tok, pos = snt[i]\n",
    "            start = text[end:-1].find(tok)\n",
    "            space = text[end:end+start]\n",
    "            cur = end+start \n",
    "            tokId += 1\n",
    "            H = {'tokId': tokId, \n",
    "                 'sntId' : sntId, \n",
    "                 'cur': end+start,\n",
    "                 'tok' : tok, \n",
    "                 'space' :space, \n",
    "                 'pos' : pos\n",
    "                }\n",
    "            # memorize tokId\n",
    "            snt[i] = (tok, tokId)\n",
    "            \n",
    "            L.append(H)\n",
    "#            print(f\"id:{tokId} cur:{cur}\\t{tok:<20}\\tend0:{end} space:{start}>{space}< {pos}\")\n",
    "    \n",
    "            end += start + len(tok) \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bde91c-f25d-4114-bf6b-7ca2ffe5a238",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde8383-9a81-4fc1-8156-b2cf47224868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "m = MeCab.Tagger('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd38b2-e1d3-4c50-8950-72270d4b8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example for merging \n",
    "L = [\n",
    " {'src': 1, 'tgt': 1},\n",
    " {'src': 2, 'tgt': 2},\n",
    " {'src': 3, 'tgt': 2},\n",
    " {'src': 2, 'tgt': 3},\n",
    " {'src': 3, 'tgt': 3},\n",
    " {'src': 3, 'tgt': 4},\n",
    " {'src': 4, 'tgt': 5},\n",
    " {'src': 5, 'tgt': 6},\n",
    " {'src': 6, 'tgt': 6},\n",
    " {'src': 7, 'tgt': 7},\n",
    " {'src': 8, 'tgt': 8},\n",
    " {'src': 9, 'tgt': 9},\n",
    " {'src': 10, 'tgt': 10},\n",
    " {'src': 11, 'tgt': 10}]\n",
    "\n",
    "\n",
    "merge_alignments_graph(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef468-00e6-4d77-b917-7c709d4c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "# test whether texts are identical\n",
    "\n",
    "# Split the strings into lists of lines\n",
    "lines1 = text1.splitlines()\n",
    "lines2 = text2.splitlines()\n",
    "\n",
    "# Use ndiff to find the differences\n",
    "diff_result = difflib.ndiff(lines1, lines2)\n",
    "\n",
    "# Print the differences\n",
    "for line in diff_result:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6b36e-ed47-4933-8853-529604cbbe46",
   "metadata": {},
   "source": [
    "## Segment Word Alignment\n",
    "- segment: sentence by sentence\n",
    "- word alignment\n",
    "- merged groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d498d4-c3ff-4a5f-a12f-7ac1ff10b3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# random word alignmet per bilingual segment \n",
    "def tokAlignment(STnt, FTnt, SntAlign):\n",
    "    s = len(STnt)\n",
    "    t = len(FTnt)\n",
    "\n",
    "    STok = [[t for t, i in s] for s in STnt]\n",
    "    FTok = [[t for t, i in s] for s in FTnt]\n",
    "    STid = [[i for t, i in s] for s in STnt]\n",
    "    FTid = [[i for t, i in s] for s in FTnt]\n",
    "\n",
    "    AG = {}\n",
    "    ag = 0\n",
    "    for aln in SntAlign:\n",
    "        sIds = [int(s)-1 for s in aln['src'].split('+')]\n",
    "        tIds = [int(s)-1 for s in aln['tgt'].split('+')]\n",
    "        \n",
    "        AG.setdefault(ag, {})\n",
    "        AG[ag]['src'] = []\n",
    "        AG[ag]['tgt'] = []\n",
    "        AG[ag]['sid'] = []\n",
    "        AG[ag]['tid'] = []\n",
    "        \n",
    "        for i in sIds : AG[ag]['src'].extend(STok[i])\n",
    "        for i in tIds : AG[ag]['tgt'].extend(FTok[i])\n",
    "        for i in sIds : AG[ag]['sid'].extend(STid[i])\n",
    "        for i in tIds : AG[ag]['tid'].extend(FTid[i])        \n",
    "        ag += 1\n",
    "\n",
    "    # random word alignment\n",
    "    L = []\n",
    "    for ag in AG:\n",
    "        AG[ag]['aln'] = []\n",
    "\n",
    "        for i in range(int((len(AG[ag]['sid']) / 1.5))) :\n",
    "            # Get a random index from the list\n",
    "            rs = random.randint(0, len(AG[ag]['sid']) - 1)\n",
    "            rt = random.randint(0, len(AG[ag]['tid']) - 1)\n",
    "\n",
    "            L.append({'src' : AG[ag]['sid'][rs], 'tgt':AG[ag]['tid'][rt]})\n",
    "\n",
    "    M = merge_alignments_graph(L)\n",
    "                \n",
    "    return M\n",
    "\n",
    "\n",
    "def sntAignment(STnt, FTnt):\n",
    "    s = len(STnt)\n",
    "    t = len(FTnt)\n",
    "    \n",
    "    L = []\n",
    "    for i in range(min(s, t)):\n",
    "        L.append({'src': i+1, 'tgt': i+1})\n",
    "    \n",
    "    if(s > t) :\n",
    "        for i in range(t, s): \n",
    "            L.append({'src': i+1, 'tgt': t})\n",
    "        \n",
    "    if(t > s) :\n",
    "        for i in range(s, t): \n",
    "            L.append({'src': s, 'tgt': i+1})\n",
    "\n",
    "    # bring into a grouped format\n",
    "    M = merge_alignments_graph(L)\n",
    "    \n",
    "    # map into xml format \n",
    "    SegAln_root = list_of_dicts_to_xml(M, root_tag='SntAlign', item_tag='Snt')\n",
    "    \n",
    "    return M\n",
    "\n",
    "\n",
    "def merge_alignments_graph(alignments):\n",
    "    \"\"\"\n",
    "    Use graph-based approach to find connected components.\n",
    "    Alignments that share src or tgt indices are in the same group.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not alignments:\n",
    "        return []\n",
    "    \n",
    "    # Build graph of connections\n",
    "    graph = defaultdict(set)\n",
    "    \n",
    "    for i, align in enumerate(alignments):\n",
    "        graph[i].add(i)\n",
    "    \n",
    "    # Connect alignments that share indices\n",
    "    for i in range(len(alignments)):\n",
    "        for j in range(i + 1, len(alignments)):\n",
    "            if (alignments[i]['src'] == alignments[j]['src'] or\n",
    "                alignments[i]['tgt'] == alignments[j]['tgt']):\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)\n",
    "    \n",
    "    # Find connected components\n",
    "    visited = set()\n",
    "    components = []\n",
    "    \n",
    "    def dfs(node, component):\n",
    "        if node in visited:\n",
    "            return\n",
    "        visited.add(node)\n",
    "        component.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            dfs(neighbor, component)\n",
    "    \n",
    "    for i in range(len(alignments)):\n",
    "        if i not in visited:\n",
    "            component = set()\n",
    "            dfs(i, component)\n",
    "            components.append(component)\n",
    "    \n",
    "    # Build merged results\n",
    "    merged = []\n",
    "    for component in components:\n",
    "        src_indices = set()\n",
    "        tgt_indices = set()\n",
    "        for idx in component:\n",
    "            src_indices.add(alignments[idx]['src'])\n",
    "            tgt_indices.add(alignments[idx]['tgt'])\n",
    "        \n",
    "        merged.append({\n",
    "            'src': sorted(src_indices),\n",
    "            'tgt': sorted(tgt_indices)\n",
    "        })\n",
    "    \n",
    "    # Sort by first src index\n",
    "    merged.sort(key=lambda x: x['src'][0])\n",
    "\n",
    "    M = []\n",
    "    for item in merged:\n",
    "        src_str = '+'.join(map(str, item['src'])) if len(item['src']) > 1 else str(item['src'][0])\n",
    "        tgt_str = '+'.join(map(str, item['tgt'])) if len(item['tgt']) > 1 else str(item['tgt'][0])\n",
    "        M.append({'src': src_str, 'tgt': tgt_str})\n",
    "    \n",
    "    return M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030534f-27b1-435b-9e02-dbf52caf5df5",
   "metadata": {},
   "source": [
    "## XML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c4955-f968-4f6a-9084-cc2f4e4e988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert list of dictionary into xml\n",
    "def list_of_dicts_to_xml(data_list, root_tag='root', item_tag='item'):\n",
    "    \"\"\"\n",
    "    Converts a list of dictionaries into an XML root,\n",
    "    placing dictionary values into attributes of XML elements.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): A list of dictionaries to convert.\n",
    "        root_tag (str): The tag name for the root element of the XML.\n",
    "        item_tag (str): The tag name for each item element in the XML.\n",
    "\n",
    "    Returns:\n",
    "        root: The XML root.\n",
    "    \"\"\"\n",
    "    root = ET.Element(root_tag)\n",
    "    for item_dict in data_list:\n",
    "        item_element = ET.SubElement(root, item_tag)\n",
    "        for key, value in item_dict.items():\n",
    "            # Convert value to string as XML attributes are strings\n",
    "            item_element.set(key, str(value))\n",
    "\n",
    "    return root\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621edb4-1e11-48cd-97a4-6099388f3c9c",
   "metadata": {},
   "source": [
    "### Events XML -> WKS XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520f671-7d39-4e04-982f-07f34c2de502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Events Token container -> list of token dictionaries\n",
    "def tokens_xml_to_dict(xml_root):\n",
    "    \"\"\"Convert list token XML to dictionary.\"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in xml_root.findall('Token'):\n",
    "        token_dict = dict(token.attrib)\n",
    "        \n",
    "        # Convert numeric strings to numbers if needed\n",
    "        if 'id' in token_dict:\n",
    "            token_dict['tokId'] = int(token_dict['id'])\n",
    "\n",
    "        tokens.append(token_dict)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2snt(tokList):\n",
    "    \"\"\"\n",
    "    Convert list of token dictionary to list of lists of sentence.\n",
    "    Assumes tokens have 'segId'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group tokens by sentence\n",
    "    S = []\n",
    "    L = []\n",
    "    segId = 0\n",
    "    for token in tokList:\n",
    "        if(token.get('segId') != segId) :\n",
    "            if(segId != 0) :\n",
    "                S.append(L)\n",
    "                L=[]\n",
    "            segId = token.get('segId')        \n",
    "        L.append((token.get('tok'), token.get('id')))\n",
    "#        print(token.get('segId'), token.get('tok'), token.get('id'))\n",
    "    \n",
    "    if(L) : S.append(L)\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75973fe1-57e7-46ef-8acd-21ecc7c5308e",
   "metadata": {},
   "source": [
    "## Simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09555c0-08ce-4332-aeaf-fa6c29380a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/data/critt/tprdb/bin/')\n",
    "import TPRDB\n",
    "import importlib\n",
    "importlib.reload(TPRDB)\n",
    "\n",
    "\n",
    "# read a study from the YAWAT folder (German)\n",
    "study = TPRDB.readYawatStudy(\"/data/critt/yawat/TPRDB/BML12/P01_T1\", verbose=0)\n",
    "#print(f\"SimAlign sessions:{len(study.keys())}\")\n",
    "\n",
    "ref, tst = TPRDB.yawat2Alignment(study)\n",
    "#print(f\"SimAlign segments:{len(ref)} {len(tst)}\\n{ref}\\n{tst}\")\n",
    "\n",
    "tst[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cac97-0c35-4d13-83e0-931239e11b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Simaligner\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "from nltk.translate import alignment_error_rate\n",
    "\n",
    "# load TPRDB library\n",
    "sys.path.append('/data/critt/tprdb/bin/')\n",
    "import TPRDB\n",
    "import importlib\n",
    "importlib.reload(TPRDB)\n",
    "\n",
    "\n",
    "# load Simaligner\n",
    "from simalign import SentenceAligner\n",
    "\n",
    "\n",
    "# \"/data/critt/yawat/TPRDB/AR19/\"\n",
    "def SimAlign(inStudy, outStudy=\"\", method=\"a\", verbose=0) :\n",
    "    # method: m, a, i\n",
    "\n",
    "    print(f\"SimAlign initiate AimAlign\\n\")\n",
    "    myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=method)\n",
    "\n",
    "    print(f\"SimAlign in:{inStudy} out:{outStudy} method:{method}\\n\")\n",
    "\n",
    "    # read a study from the YAWAT folder (German)\n",
    "    study = TPRDB.readYawatStudy(inStudy, verbose=0)\n",
    "    print(f\"SimAlign sessions:{len(study.keys())}\")\n",
    "\n",
    "    # separate study into list of NLTK alignments with reference and test set\n",
    "    ref, tst = TPRDB.yawat2Alignment(study)\n",
    "    print(f\"SimAlign segments:{len(ref)}\")\n",
    "\n",
    "    #run alignment\n",
    "    SimA = simAlignment(tst, method, myaligner, verbose=0)\n",
    "\n",
    "    print(\"SimAlign: transitiveAlignment\")\n",
    "    # transitive mapping \n",
    "    SimAT = TPRDB.transitiveAlignment(SimA, verbose = 0)\n",
    "\n",
    "    # add alignments to study under feature name \"SimA\"\n",
    "    TPRDB.alignment2Yawat(study, SimAT, feat=\"SimAT\")\n",
    "\n",
    "    # write alignments to study\n",
    "    if(inStudy == '') : outStudy = inStudy\n",
    "    print(f\"SimAlign: writeYawatStudy {outStudy}\")\n",
    "    TPRDB.writeYawatStudy(outStudy, study, feat=\"SimAT\")\n",
    "\n",
    "\n",
    "# simalign\n",
    "# simalign\n",
    "\n",
    "def simAlignment(ALN, method, myaligner, verbose = 0):\n",
    "    R = []\n",
    "\n",
    "    for seg in range(len(ALN)):\n",
    "        aln = []\n",
    "        mot = ALN[seg].mots\n",
    "        word = ALN[seg].words\n",
    "        if(verbose): print(f\"simAlignment: {seg} from {len(ALN)}\")\n",
    "        if((seg % 10) == 0) : print(f\"simAlignment: {seg} from {len(ALN)}\")\n",
    "\n",
    "        if(len(mot) == 0 or len(word) == 0):\n",
    "            if(verbose): print(f\"Unaligned: {word}\\n{mot}\")\n",
    "            R.append(AlignedSent(word, mot, Alignment(aln)))\n",
    "        else :\n",
    "            aln = myaligner.get_word_aligns(word, mot)\n",
    "            for a in aln:\n",
    "                R.append(AlignedSent(word, mot, Alignment(aln[a])))\n",
    "                break\n",
    "    return R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe8ece-fa86-4067-9245-e1536eaef4ed",
   "metadata": {},
   "source": [
    "# Events to Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a3346-850e-4f13-98d1-9ea5193bb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import io\n",
    "\n",
    "path = \"/data/critt/tprdb/TPRDB\"\n",
    "studies = ['BML12', 'SG12', 'RUC17']\n",
    "verbose = 2\n",
    "    \n",
    "for study in studies:\n",
    "    files = glob.glob(f\"{path}/{study}/Events/*Event.xml\")\n",
    "    if(verbose): print(f\"Reading:{study}\\twith {len(files)} files\")\n",
    "    try:\n",
    "        os.mkdir(f\"{path}/{study}/WKS/\")\n",
    "    except FileExistsError:\n",
    "        print(f\"\\tDirectory WKS already exists.\")\n",
    "\n",
    "    n = 0\n",
    "    for fn in sorted(files):\n",
    "        root, extension = os.path.splitext(fn)\n",
    "        wks = os.path.basename(root).removesuffix(\".Event\")\n",
    "        out = f\"{path}/{study}/WKS/{wks}.xml\"\n",
    "        n += 1\n",
    "        \n",
    "        if os.path.exists(out):\n",
    "            if(verbose): print(f\"  {n}\\tExists: {out}\")\n",
    "            continue\n",
    "            \n",
    "        if(verbose):  print(f\"  {n}\\tOutput: {out}\")\n",
    "            \n",
    "        try:\n",
    "            WorkSpace_root = Events2WKS(fn) \n",
    "            ET.indent(WorkSpace_root, space='  ')  # 2 spaces\n",
    "            with open(out, \"w\") as f:\n",
    "                print(ET.tostring(WorkSpace_root, encoding='unicode'), file=f)\n",
    "\n",
    "        except FileExistsError:\n",
    "            print(f\"\\tError in XML File.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaa6fb-01ab-43de-8c26-bf7897420166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = \"/data/critt/tprdb/TPRDB/ENJA15/Events/P01_T1.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB/BML12/Events/P22_P5.Event.xml\"\n",
    "fn = \"/data/critt/tprdb/TPRDB//RUC17/Events/P02_P3.Event.xml\"\n",
    "\n",
    "\n",
    "WorkSpace_root = Events2WKS(fn, Verbose=1) \n",
    "\n",
    "##################################################################\n",
    "# pretty-print WorkSpace_root\n",
    "ET.indent(WorkSpace_root, space='  ')  # 2 spaces\n",
    "print(ET.tostring(WorkSpace_root, encoding='unicode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6a5bd-c047-4edf-817a-8f70b6d23e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Events2WKS(fn, Verbose = 0) :\n",
    "    \n",
    "    ### Root of the WorkSpace XML output file\n",
    "    WorkSpace_root = ET.Element(\"WorkSpace\")\n",
    "    \n",
    "    if(Verbose) : print(f\"Events2WKS 1\")\n",
    "    \n",
    "    ### Read Events-XML file\n",
    "    events = ET.parse(fn)\n",
    "    if(Verbose) : print(f\"Events2WKS 2\")\n",
    "    # Root of the Translog XML input file\n",
    "    events_root = events.getroot()\n",
    "    if(Verbose) : print(f\"Events2WKS 2\")\n",
    "    \n",
    "    # get Source and Target Languages\n",
    "    e = events_root.find('.//Languages')\n",
    "    SL = e.get('source') \n",
    "    TL = e.get('target')\n",
    "\n",
    "    if(Verbose) : print(f\"Events2WKS: SL:{SL} TL:{TL}\")\n",
    "        \n",
    "    ##################################################################\n",
    "    \n",
    "    def EventsToken(tag, lng, Verbose=0) :\n",
    "        \n",
    "        e = events_root.find(f\".//{tag}\")\n",
    "        token = tokens_xml_to_dict(e)\n",
    "        \n",
    "        if(Verbose) : print(f\"EventsToken: lng:{lng} No:{len(token)}\")\n",
    "\n",
    "        # Reconstruct Sentence from Tokens \n",
    "        snt = tokens2snt(token)\n",
    "        \n",
    "        # get additional features from Stanza \n",
    "        # add features to list of STokens \n",
    "        tokFeats = stanzaFeatures(snt, lng, token)\n",
    "        \n",
    "        # convert token Dictionary to xml\n",
    "        token_root = list_of_dicts_to_xml(tokFeats, root_tag=tag, item_tag='Token')\n",
    "        \n",
    "        # assign source language \n",
    "        token_root.set('language', str(lng))\n",
    "        return token_root\n",
    "    \n",
    "    ######################################\n",
    "    # append FT tokenization to WorkSpace root \n",
    "    Root = EventsToken('SourceToken', SL, Verbose=Verbose)\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    # append FT tokenization to WorkSpace root \n",
    "    Root = EventsToken('FinalToken', TL, Verbose=Verbose)\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ######################################\n",
    "    e = events_root.find('.//Alignment')\n",
    "    L = []\n",
    "    for token in e.findall('Align'):\n",
    "        d = dict(token.attrib)\n",
    "        d['src'] = d['sid']\n",
    "        d['tgt'] = d['tid']\n",
    "        L.append(d)\n",
    "     \n",
    "    M = merge_alignments_graph(L)\n",
    "    if(Verbose) : print(f\"Alignment: {len(M)}\")\n",
    "    \n",
    "    # map into xml format \n",
    "    Root = list_of_dicts_to_xml(M, root_tag='TokAlign', item_tag='Tok')\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ######################################\n",
    "    e = events_root.find('.//Salignment')\n",
    "    L = []\n",
    "    for token in e.findall('Salign'):\n",
    "        d = dict(token.attrib)\n",
    "        L.append(d)\n",
    "     \n",
    "    M = merge_alignments_graph(L)\n",
    "    if(Verbose) : print(f\"Salignment: {len(M)}\")\n",
    "    \n",
    "    # map into xml format \n",
    "    Root = list_of_dicts_to_xml(M, root_tag='SntAlign', item_tag='Snt')\n",
    "    WorkSpace_root.append(Root)\n",
    "    \n",
    "    ##################################################################\n",
    "    e = events_root.find('.//Modifications')\n",
    "    \n",
    "    L = []\n",
    "    for token in e.findall('Mod'):\n",
    "        d = dict(token.attrib)\n",
    "        # rename sid and tid\n",
    "        d['src'] = d.pop('sid')\n",
    "        d['tgt'] = d.pop('tid')\n",
    "    \n",
    "        L.append(d)\n",
    "\n",
    "    if(Verbose) : print(f\"Modifications: {len(L)}\")\n",
    "\n",
    "    Root = list_of_dicts_to_xml(L, root_tag='Modifications', item_tag='Mod')\n",
    "    WorkSpace_root.append(Root)\n",
    "     \n",
    "    \n",
    "    ##################################################################\n",
    "    e = events_root.find('.//Fixations')\n",
    "    L = []\n",
    "    for token in e.findall('Fix'):\n",
    "        d = dict(token.attrib)\n",
    "        \n",
    "        # rename sid and tid\n",
    "        d['src'] = d.pop('sid')\n",
    "        d['tgt'] = d.pop('tid')\n",
    "        L.append(d)\n",
    "\n",
    "    if(Verbose) : print(f\"Fixations: {len(L)}\")\n",
    "        \n",
    "    Root = list_of_dicts_to_xml(L, root_tag='Fixations', item_tag='Fix')\n",
    "    WorkSpace_root.append(Root)    \n",
    "    \n",
    "    ##################################################################\n",
    "    ### Segment open-closing\n",
    "    #  <Segments>\n",
    "    #    <Seg sntId=\"1\" open=\"72952\" close=\"89436\" />\n",
    "    e = events_root.find('.//Segments')\n",
    "    L = []\n",
    "    for token in e.findall('Seg'):\n",
    "        d = dict(token.attrib)\n",
    "        L.append(d)\n",
    "        \n",
    "    if(Verbose) : print(f\"Segments: {len(L)}\")\n",
    "\n",
    "    Root = list_of_dicts_to_xml(L, root_tag='SntEdits', item_tag='Snt')\n",
    "    WorkSpace_root.append(Root)\n",
    "\n",
    "    return WorkSpace_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8633f-60f5-4595-a370-09f4cecc3b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
